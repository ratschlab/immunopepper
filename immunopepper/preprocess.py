"""Contains functions to parse and preprocess information from the input file"""
import sys
import os
import h5py
import logging
import multiprocessing as mp
import numpy as np
import pandas as pd
import pickle

from immunopepper.io_ import decode_utf8
from immunopepper.namedtuples import CountInfo
from immunopepper.namedtuples import GeneInfo
from immunopepper.namedtuples import GeneTable
from immunopepper.namedtuples import ReadingFrameTuple
from immunopepper.utils import encode_chromosome
from immunopepper.utils import find_overlapping_cds_simple
from immunopepper.utils import get_successor_list
from immunopepper.utils import leq_strand
from immunopepper.utils import pool_initializer

def genes_preprocess_batch(genes, gene_idxs, gene_cds_begin_dict, all_read_frames=False):

    gene_info = []
    for gene in genes:
        gene.from_sparse()
        assert (gene.strand in ["+", "-"])
        assert (len(gene.transcripts) == len(gene.exons))

        # Ignore genes that have no CDS annotated in annotated frame mode
        if (not all_read_frames) and (gene.name not in gene_cds_begin_dict):
            gene_info.append(None)
            continue

        vertex_succ_list = get_successor_list(gene.splicegraph.edges, gene.splicegraph.vertices, gene.strand)
        if gene.strand == "+":
            vertex_order = np.argsort(gene.splicegraph.vertices[0, :])
        else:  # gene.strand=="-"
            vertex_order = np.argsort(gene.splicegraph.vertices[1, :])[::-1]

        # get the reading_frames
        reading_frames = {}
        vertex_len_dict = {}
        if not all_read_frames:
            for idx in vertex_order:
                reading_frames[idx] = set()
                v_start = gene.splicegraph.vertices[0, idx]
                v_stop = gene.splicegraph.vertices[1, idx]
                cds_begins = find_overlapping_cds_simple(v_start, v_stop, gene_cds_begin_dict[gene.name], gene.strand)
                vertex_len_dict[idx] = v_stop - v_start

                # Initialize reading regions from the CDS transcript annotations
                for cds_begin in cds_begins:
                    line_elems = cds_begin[2]
                    cds_strand = line_elems[6]
                    assert (cds_strand == gene.strand)
                    cds_phase = int(line_elems[7])
                    cds_left = int(line_elems[3])-1
                    cds_right = int(line_elems[4])

                    #TODO: need to remove the redundance of (cds_start, cds_stop, item)
                    if gene.strand == "-":
                        cds_right_modi = max(cds_right - cds_phase,v_start)
                        cds_left_modi = v_start
                        n_trailing_bases = cds_right_modi - cds_left_modi
                    else:
                        cds_left_modi = min(cds_left + cds_phase,v_stop)
                        cds_right_modi = v_stop
                        n_trailing_bases = cds_right_modi - cds_left_modi

                    read_phase = n_trailing_bases % 3
                    reading_frames[idx].add(ReadingFrameTuple(cds_left_modi, cds_right_modi, read_phase))
        gene.to_sparse()
        gene_info.append(GeneInfo(vertex_succ_list, vertex_order, reading_frames, vertex_len_dict, gene.splicegraph.vertices.shape[1]))

    return gene_info, gene_idxs, genes


def genes_preprocess_all(genes, gene_cds_begin_dict, parallel=1, all_read_frames=False):
    """ Preprocess the gene and generate new attributes under gene object
        Modify the gene object directly

    Parameters
    ----------
    genes: List[Object]. List of gene objects. The object is generated by SplAdder
    gene_cds_begin_dict: Dict. str -> List(int) From gene name to list of cds start positions
    """

    if parallel > 1:
        global genes_info
        global genes_modif
        global cnt
        genes_info = np.zeros((genes.shape[0],), dtype=object)
        genes_modif = np.zeros((genes.shape[0],), dtype=object)
        cnt = 0
        def update_gene_info(result):
            global genes_info
            global cnt
            global genes_modif
            assert(len(result[0]) == len(result[2]))
            for i,tmp in enumerate(result[0]):
                if cnt > 0 and cnt % 1000 == 0:
                    sys.stdout.write('.')
                    if cnt % 10000 == 0:
                        sys.stdout.write('%i/%i\n' % (cnt, genes.shape[0]))
                    sys.stdout.flush()
                cnt += 1
                genes_info[result[1][i]] = tmp
                genes_modif[result[1][i]] = result[2][i]
            del result

        pool = mp.Pool(processes=parallel, initializer=pool_initializer)
        for i in range(0, genes.shape[0], 100):
            gene_idx = np.arange(i, min(i + 100, genes.shape[0]))
            _ = pool.apply_async(genes_preprocess_batch, args=(genes[gene_idx], gene_idx, gene_cds_begin_dict, all_read_frames,), callback=update_gene_info)
        pool.close()
        pool.join()
    else:
        genes_info = genes_preprocess_batch(genes, np.arange(genes.shape[0]), gene_cds_begin_dict, all_read_frames)[0]
        genes_modif = genes
    return genes_info, genes_modif


def preprocess_ann(ann_path):
    """ Extract information from annotation file (.gtf, .gff and .gff3)

    Parameters
    ----------
    ann_path: str. Annotation file path

    Returns
    -------
    gene_table: NamedTuple.store the gene-transcript-cds mapping tables derived
        from .gtf file. has attribute ['gene_to_cds_begin', 'ts_to_cds', 'gene_to_cds']
    chromosome_set: set. Store the chromosome naming.
    """
    transcript_to_gene_dict = {}    # transcript -> gene id
    gene_to_transcript_dict = {}    # gene_id -> list of transcripts
    transcript_to_cds_dict = {}     # transcript -> list of CDS exons
    transcript_cds_begin_dict = {}  # transcript -> first exon of the CDS
    gene_cds_begin_dict = {}        # gene -> list of first CDS exons

    file_type = ann_path.split('.')[-1]
    chromesome_set = set()
    # collect information from annotation file
    for line in open(ann_path, 'r'):
        if line[0] == '#':
            continue
        item = line.strip().split('\t')
        chromesome_set.add(item[0])
        feature_type = item[2]
        attribute_item = item[-1]
        attribute_dict = attribute_item_to_dict(attribute_item, file_type, feature_type)
        # store relationship between gene ID and its transcript IDs
        if feature_type in ['transcript', 'mRNA']:
            gene_id = attribute_dict['gene_id']
            transcript_id = attribute_dict['transcript_id']
            if attribute_dict['gene_type'] != 'protein_coding' or attribute_dict['transcript_type']  != 'protein_coding':
                continue
            assert (transcript_id not in transcript_to_gene_dict)
            transcript_to_gene_dict[transcript_id] = gene_id
            if gene_id in gene_to_transcript_dict and transcript_id not in gene_to_transcript_dict[gene_id]:
                gene_to_transcript_dict[gene_id].append(transcript_id)
            else:
                gene_to_transcript_dict[gene_id] = [transcript_id]

        # Todo python is 0-based while gene annotation file(.gtf, .vcf, .maf) is one based
        elif feature_type == "CDS":
            parent_ts = attribute_dict['transcript_id']
            strand_mode = item[6]
            cds_left = int(item[3])-1
            cds_right = int(item[4])
            frameshift = int(item[7])
            if parent_ts in transcript_to_cds_dict:
                transcript_to_cds_dict[parent_ts].append((cds_left, cds_right, frameshift))
            else:
                transcript_to_cds_dict[parent_ts] = [(cds_left, cds_right, frameshift)]
            if strand_mode == "+" :
                cds_start, cds_stop = cds_left, cds_right
            else:
                cds_start, cds_stop = cds_right, cds_left

            # we only consider the start of the whole CoDing Segment
            if parent_ts not in transcript_cds_begin_dict or \
               leq_strand(cds_start, transcript_cds_begin_dict[parent_ts][0], strand_mode):
                transcript_cds_begin_dict[parent_ts] = (cds_start, cds_stop, item)

    # collect first CDS exons for all transcripts of a gene
    for ts_key in transcript_to_gene_dict:
        target_gene = transcript_to_gene_dict[ts_key]
        if target_gene not in gene_cds_begin_dict:
            gene_cds_begin_dict[target_gene] = []
        if ts_key in transcript_cds_begin_dict:
            gene_cds_begin_dict[target_gene].append(transcript_cds_begin_dict[ts_key])

    # sort list of CDS exons per transcript
    for ts_key in transcript_to_cds_dict:
        transcript_to_cds_dict[ts_key] = sorted(transcript_to_cds_dict[ts_key], key=lambda coordpair: coordpair[0])

    genetable = GeneTable(gene_cds_begin_dict, transcript_to_cds_dict, gene_to_transcript_dict)
    return genetable,chromesome_set


def attribute_item_to_dict(a_item, file_type, feature_type):
    """  From attribute item in annotation file to get corresponding dictionary

    Parameters
    ----------
    a_item: str. attribute item
    file_type: str. Choose from {'gtf', 'gff', 'gff3'}
    feature_type: str. Extract other fields. We only
        consider 'CDS', 'mRNA' and 'transcript'

    Returns
    -------
    gtf_dict: dict. store all the necessary data

    """
    gtf_dict = {}
    if file_type.lower() == 'gtf':
        attribute_list = a_item.split('; ')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split(' ')
            gtf_dict[pair[0]] = pair[1][1:-1]
    elif file_type.lower() == 'gff3':
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gtf_dict[pair[0]] = pair[1]
    elif file_type.lower() == 'gff':
        gff_dict = {}
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gff_dict[pair[0]] = pair[1]  # delete "", currently now work on level 2
        if feature_type == 'CDS':
            gtf_dict['transcript_id'] = gff_dict['Parent']
        elif feature_type in {'mRNA', 'transcript'}:  # mRNA or transcript
            gtf_dict['gene_id'] = gff_dict['geneID']
            gtf_dict['transcript_id'] = gff_dict['ID']
            gtf_dict['gene_type'] = gff_dict['gene_type']
            gtf_dict['transcript_type'] = gff_dict['transcript_type']

    return gtf_dict


def search_edge_metadata_segmentgraph(gene, coord, edge_idxs=None, edge_counts=None, cross_graph_expr=None):
    """Given the ordered edge coordinates of the edge, return expression information of the edge

    Parameters
    ----------
    gene: Object. Generated by SplAdder
    coord: np.array of length 4. Sorted coordinates of 4 positions in ascending order
    countinfo: NamedTuple, contains SplAdder count info
    Idx: Namedtuple, has attribute idx.gene and idx.sample
    edge_idxs: np.array, containing the edge index values for the current gene
    egde_counts: np.array, containing the edge count values for the current gene

    Returns
    -------
    count: tuple of float. Expression level for the given edges.
    """
    def get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts=None):
        a = np.searchsorted(segmentgraph.segments[1, :], sorted_pos[1])
        b = np.searchsorted(segmentgraph.segments[0, :], sorted_pos[2])
        if a < b:
            idx = np.ravel_multi_index([a, b], segmentgraph.seg_edges.shape)
        else:
            idx = np.ravel_multi_index([b, a], segmentgraph.seg_edges.shape)
        cidx = np.searchsorted(edge_idxs, idx)
        if len(edge_counts.shape) > 1:
            counts = edge_counts[cidx,:]
        else:
            counts = np.array([edge_counts[cidx]])
        return counts

    segmentgraph = gene.segmentgraph
    sorted_pos = np.sort(np.array([coord.start_v1, coord.stop_v1, coord.start_v2, coord.stop_v2]))

    count = get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts)

    if coord.start_v3 is None:
        edges_res = [(c_,) for c_ in count] #(count,)
    else:
        sorted_pos = np.sort(np.array([coord.start_v2, coord.stop_v2, coord.start_v3, coord.stop_v3]))
        count2 = get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts)
        edges_res = [(c_, v_) for c_, v_ in zip(count, count2)]

    if not cross_graph_expr:
        edges_res = edges_res[0]

    return edges_res

def parse_gene_metadata_info(h5fname, sample_list, cross_graph_expr):
    """ Parse the count file

    Parameters
    ----------
    h5fname: str. .count.h5f file
    sample_list: List(str). List of samples.

    Returns
    -------
    countinfo: Namedtuple. Store all the counts information. Has attributes:
        'sample_idx_dict' --> dict from sample name to index
        'gene_idx_dict' --> dict from gene name to index
        'gene_ids_segs' --> array containing segment-geneID relation
        'gene_ids_edges' --> array containing edge-geneID relation
        'h5fname' --> HDF5 file name
    """

    # the SplAdder count hdf5 file has the following structure
    #   h5f["strains"] --> sample names
    #   h5f["segments"] --> segment expression (rows: segments, columns: samples)
    #   h5f["edges"] --> edge expression (rows: edges, columns: samples)
    #   h5f["edge_idx"] --> multi-row index encoding the edge in the splice graph (rows:
    h5f = h5py.File(h5fname, 'r')
    assert (h5f["strains"].shape[0] == h5f["segments"].shape[1])
    assert (h5f["gene_ids_segs"].size ==  h5f["segments"].shape[0])
    assert (h5f["gene_ids_edges"].size == h5f["edges"].shape[0])

    ### create a sample name dictionary mapping sample names to indices
    count_names = h5f['strains'][:] if len(h5f['strains'].shape) == 1 else h5f['strains'][:, 0]
    sample_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(count_names)])

    ### create a gene name dictionary mapping gene names to indices
    gene_names = h5f['gene_names'][:] if len(h5f['gene_names'].shape) == 1 else h5f['gene_names'][:, 0]
    gene_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(gene_names)])

    gene_ids_segs = h5f['gene_ids_segs'][:] if len(h5f['gene_ids_segs'].shape) == 1 else h5f['gene_ids_segs'][:, 0]
    gene_ids_edges = h5f['gene_ids_edges'][:] if len(h5f['gene_ids_edges'].shape) == 1 else h5f['gene_ids_edges'][:, 0]

    ### segs
    gene_ids_segs_u, gene_ids_segs_idx = np.unique(gene_ids_segs, return_index=True)
    gene_ids_segs_idx_last = np.r_[gene_ids_segs_idx[1:], [gene_ids_segs.shape[0]]]
    gene_id_to_segrange = dict()
    for i, g in enumerate(gene_ids_segs_u):
        gene_id_to_segrange[g] = (gene_ids_segs_idx[i], gene_ids_segs_idx_last[i])
    ### edges
    gene_ids_edges_u, gene_ids_edges_idx = np.unique(gene_ids_edges, return_index=True)
    gene_ids_edges_idx_last = np.r_[gene_ids_edges_idx[1:], [gene_ids_edges.shape[0]]]
    gene_id_to_edgerange = dict()
    for i, g in enumerate(gene_ids_edges_u):
        gene_id_to_edgerange[g] = (gene_ids_edges_idx[i], gene_ids_edges_idx_last[i])

    countinfo = CountInfo(sample_idx_dict,
                          gene_idx_dict,
                          gene_id_to_segrange,
                          gene_id_to_edgerange,
                          h5fname)
    h5f.close()
    if cross_graph_expr and sample_list:
        # Retrieve count id matching input samples
        matching_count_ids = np.array([s_idx for input_sample in sample_list for s_idx, graph_sample in enumerate(count_names)
                                  if graph_sample.decode() == input_sample ])
        if countinfo is not None and len(matching_count_ids) == 0:
            logging.error("Output samples do not match count file samples")
            sys.exit(1)
    else:
        matching_count_ids = None

    matching_count_samples = [n.decode('utf8') for n in count_names]
    return countinfo, matching_count_samples, matching_count_ids


def parse_mutation_from_vcf(mutation_tag, vcf_path, output_dir='', heter_code=0, mut_pickle=False, mutation_sample=None, graph_to_mutation_samples={}):
    """Extract germline mutation information from the given vcf file and vcf.h5 file

    Parameters
    ----------
    vcf_path: str, vcf file path
    output_dir: str, path to vcf pickle output directory
    heter_code: int (0 or 2). specify which number represents heter alle.
        0: 0-> homozygous alternative(1|1), 1-> heterozygous(0|1,1|0) 2->homozygous reference(0|0)
        2: 0-> homozygous reference(0|0), 1-> heterozygous(0|1,1|0) 2->homozygous alternative(1|1)
    mut_pickle: bool, flag indicating whether to pickle mutation info to disk
    target_sample_list: list, list of samples

    Returns
    -------
    mut_dict: with key (sample, chromo) and values (var_dict)
    """
    mutation_to_graph_samples = { file_sample: target_sample for target_sample, file_sample in graph_to_mutation_samples.items()}
    vcf_pkl_file = os.path.join(output_dir, '{}_vcf.pickle'.format(mutation_tag))
    if mut_pickle:
        if os.path.exists(vcf_pkl_file):
            f = open(vcf_pkl_file, 'rb')
            mutation_dic = pickle.load(f)
            logging.info("Use pickled vcf mutation dict in {}".format(vcf_pkl_file))
            return mutation_dic

    file_type = vcf_path.split('.')[-1]
    if file_type == 'h5': # hdf5 filr
        mutation_dic = parse_mutation_from_vcf_h5(vcf_path, mutation_sample, heter_code, graph_to_mutation_samples, mutation_to_graph_samples)
        logging.info("Get germline mutation dict from h5 file in {}. No pickle file created".format(vcf_path))
        return mutation_dic
    else: # vcf text file
        f = open(vcf_path,'r')
        lines = f.readlines()
        mutation_dic = {}
        for line in lines:
            if line.strip()[:2] == '##':  # annotation line
                continue
            if line.strip()[0] == '#':  # head line
                fields = line.strip().split('\t')
                vcf_sample_set = fields[9:]

                check_mutation_sample_presence(mutation_sample, vcf_sample_set, mutation_to_graph_samples)
                continue
            items = line.strip().split('\t')
            var_dict = {}
            chr = items[0]
            pos= int(items[1])-1
            var_dict['ref_base'] = items[3]
            var_dict['mut_base'] = items[4]
            var_dict['qual'] = items[5]
            var_dict['filter'] = items[6]
            if len(var_dict['ref_base']) == len(var_dict['mut_base']):  # only consider snp for now
                for i, file_sample in enumerate(vcf_sample_set):
                    if items[9+i].split(':')[0] in {'1|1' ,'1|0', '0|1', '0/1', '1/0', '1/1'}:
                        if file_sample not in mutation_to_graph_samples.keys():
                            mutation_to_graph_samples[file_sample] = file_sample
                        if (mutation_to_graph_samples[file_sample] ,chr) in list(mutation_dic.keys()):
                            mutation_dic[(mutation_to_graph_samples[file_sample],chr)][int(pos)] = var_dict
                        else:
                            mutation_dic[(mutation_to_graph_samples[file_sample],chr)] = {}
                            mutation_dic[(mutation_to_graph_samples[file_sample], chr)][int(pos)] = var_dict
    if mut_pickle:
        f_pkl =open(vcf_pkl_file,'wb')
        pickle.dump(mutation_dic,f_pkl)
        logging.info("create vcf pickled mutation dict for next time's use in {}".format(vcf_pkl_file))

    return mutation_dic


def parse_mutation_from_vcf_h5(h5_vcf_path, mutation_sample, heter_code=0, graph_to_mutation_samples={}, mutation_to_graph_samples={}):
    """
    Extract germline mutation information from given vcf h5py file.

    Parameters
    ----------
    h5_vcf_path: str, vcf file path
    target_sample_list: list of str, list for sample name
    heter_code: int (0 or 2). specify which number represents heter alle.
        0: 0-> homozygous alternative(1|1), 1-> heterozygous(0|1,1|0) 2->homozygous reference(0|0)
        2: 0-> homozygous reference(0|0), 1-> heterozygous(0|1,1|0) 2->homozygous alternative(1|1)

    Returns
    -------
    mut_dict: with key (sample, chromo) and values (var_dict)

    """
    a = h5py.File(h5_vcf_path,'r')
    mut_dict = {}
    vcf_sample_set = [decode_utf8(item) for item in a['gtid']]
    check_mutation_sample_presence(mutation_sample, vcf_sample_set, mutation_to_graph_samples)
    col_id = [i for (i, item) in enumerate(vcf_sample_set) if item == mutation_sample][0]
    row_id = np.where(np.logical_or(a['gt'][:,col_id] == heter_code,a['gt'][:,col_id] == 1))[0]
    for irow in row_id:
        chromo = encode_chromosome(a['pos'][irow,0])
        pos = a['pos'][irow,1]-1
        mut_base = decode_utf8(a['allele_alt'][irow])
        ref_base = decode_utf8(a['allele_ref'][irow])
        var_dict = {"mut_base":mut_base,"ref_base":ref_base}
        if (mutation_sample,chromo) in mut_dict:
            mut_dict[(mutation_sample,chromo)][pos] = var_dict
        else:
            mut_dict[(mutation_sample,chromo)] = {}
            mut_dict[(mutation_sample,chromo)][pos] = var_dict
    logging.info("TEST {}".format(len(mut_dict)))
    return mut_dict


def check_mutation_sample_presence(mutation_sample, maf_or_vcf_sample_set, mutation_to_graph_samples):
    if mutation_sample not in maf_or_vcf_sample_set:
        logging.error("Target mutation sample {} is not found in mutation/variant file."
                        " Please check --mutation-sample or consider using --sample-name-map.".format(mutation_sample))
        logging.error("Samples in mutation/variant file are: {}".format(maf_or_vcf_sample_set))
        sys.exit(1)
    for target_sample in mutation_to_graph_samples:
        if target_sample not in maf_or_vcf_sample_set:
            logging.error("Sample {} to extract and pickle not found in mutation/variant file. "
                          " Please check --pickle-samples or consider using --sample-name-map.".format(target_sample))
            logging.error("Samples in mutation/variant file are: {}".format(maf_or_vcf_sample_set))
            sys.exit(1)

def parse_mutation_from_maf(mutation_tag, mutation_sample, maf_path, output_dir='',
                            mut_pickle=False, graph_to_mutation_samples={}):
    """
    Extract somatic mutation information from given maf file.

    Parameters
    ----------
    maf_path: str, maf file path
    output_dir: str, save a pickle for maf_dict to save preprocess time
    mut_pickle: bool, flag indicating whether to pickle mutation info to disk

    Returns
    -------
    mut_dict: with key (sample, chromo) and values (var_dict)

    """
    mutation_to_graph_samples = { file_sample: target_sample for target_sample, file_sample in graph_to_mutation_samples.items()}
    maf_pkl_file = os.path.join(output_dir, '{}_maf.pickle'.format(mutation_tag))
    if mut_pickle:
        if os.path.exists(maf_pkl_file):
            f = open(maf_pkl_file,'rb')
            mutation_dic = pickle.load(f)
            logging.info("Use pickled maf mutation dict in {}".format(maf_pkl_file))
            maf_sample_set = set()
            for sample_id,chr in mutation_dic:
                maf_sample_set.add(sample_id)
            check_mutation_sample_presence(mutation_sample, maf_sample_set, mutation_to_graph_samples)
            return mutation_dic

    f = open(maf_path)
    lines = f.readlines()
    mutation_dic = {}
    maf_sample_set = set()
    for i,line in enumerate(lines[1:]):
        items = line.strip().split('\t')
        if items[9] == 'SNP':  # only consider snp
            file_sample = items[15]
            maf_sample_set.add(file_sample)
            chr = items[4]
            pos = int(items[5])-1
            var_dict = {}
            var_dict['ref_base'] = items[10]
            var_dict['mut_base'] = items[12]
            var_dict['strand'] = items[7]
            var_dict['variant_Classification'] = items[8]
            var_dict['variant_Type'] = items[9]
            if file_sample not in mutation_to_graph_samples.keys():
                mutation_to_graph_samples[file_sample] = file_sample
            if (mutation_to_graph_samples[file_sample],chr) in list(mutation_dic.keys()):
                mutation_dic[((mutation_to_graph_samples[file_sample],chr))][int(pos)] = var_dict
            else:
                mutation_dic[((mutation_to_graph_samples[file_sample], chr))] = {}
                mutation_dic[((mutation_to_graph_samples[file_sample],chr))][int(pos)] = var_dict
    if mut_pickle:
        f_pkl =open(maf_pkl_file,'wb')
        pickle.dump(mutation_dic,f_pkl)
        logging.info("create maf pickled mutation dict for next time's use in {}".format(maf_pkl_file))

    check_mutation_sample_presence(mutation_sample, maf_sample_set, mutation_to_graph_samples)
    return mutation_dic

#todo: support tsv file in the future
def parse_junction_meta_info(h5f_path):
    """ Extract introns of interest from given h5py file

    Parameters
    ----------
    h5f_path: str, h5py file path

    Returns
    -------
    junction_dict: dict, key (chromosome id), value (set of coordinate pairs)

    """
    if h5f_path is None:
        return None
    else:
        h5f = h5py.File(h5f_path,'r')
        chrms = h5f['chrms'][:]
        pos = h5f['pos'][:].astype('str')
        strand = h5f['strand'][:]
        junction_dict = {}

        for i,ichr in enumerate(chrms):
            try:
                junction_dict[decode_utf8(ichr)].add(':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])]))
            except KeyError:
                junction_dict[decode_utf8(ichr)] = set([':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])])])
    return junction_dict


def parse_gene_choices(genes_interest, process_chr, process_num, complexity_cap, disable_process_libsize, graph_data):

    if process_num == 0:  # Default process all genes
        num = len(graph_data)
    else:
        num = process_num
        if num > len(graph_data):
            logging.error(
                "Requested more genes than available in splice graph. Check argument --process_num")
            sys.exit(1)
        graph_data = graph_data[:num]
        disable_process_libsize = True
        logging.warning(
            "Developer mode, processing the first {} genes. Library size will not be outputted".format(process_num))

    if genes_interest is not None:
        genes_interest = pd.read_csv(genes_interest, header=None)[0].tolist()
        if len(np.array([gene for gene in graph_data if gene.name in genes_interest])) == 0:
            logging.error("Gene of interest not found in splicing graph. Check argument --genes_interest")
            sys.exit(1)
    else:
        genes_interest = [gene.name for gene in graph_data]

    if process_chr is not None:
        gene_with_chr = [gene.name for gene in graph_data if gene.chr in process_chr]
        if len(gene_with_chr) == 0:
            logging.error(
                "Chromosome {} not found in splicing graph. Check argument --process_chr".format(
                    process_chr))
            sys.exit(1)
        genes_interest = [gene for gene in genes_interest if gene in gene_with_chr]
        if len(genes_interest) == 0:
            logging.error(
                "Gene of interest and chromosome of interest do not match. Check argument --genes_interest, --process_chr")
            sys.exit(1)


    if complexity_cap is None or complexity_cap==0: #Default no complexity cap
        complexity_cap=np.inf

    return graph_data, genes_interest, num, complexity_cap, disable_process_libsize


def parse_output_samples_choices(arg, countinfo, matching_count_ids, matching_count_samples):
    '''handle output_sample relatively to output mode '''

    if arg.cross_graph_expr:
        if countinfo:
            process_output_samples = ['cohort']
            # If output samples requested, look for sample ids in count file
            if arg.output_samples:
                arg.output_samples = np.array(arg.output_samples)[np.argsort(matching_count_ids)]
                arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                      for output_sample in  arg.output_samples]
                output_samples_ids = matching_count_ids[np.argsort(matching_count_ids)]
            # If no output samples requested, take all samples in countfile
            else:
                arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                      for output_sample in  matching_count_samples]
                output_samples_ids = None
        else:
            logging.error("Count file must be specified in --cross-graph-exp mode")
            sys.exit(1)
    else:
        if arg.output_samples:
            process_output_samples = arg.output_samples
            output_samples_ids = None
        else:
            logging.error("--arg.output_samples must be specified in single sample mode")
            sys.exit(1)
    return process_output_samples, output_samples_ids