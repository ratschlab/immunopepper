"""Contains functions to parse and preprocess information from the input file"""
import sys
import os
import h5py
import logging
import multiprocessing as mp
import numpy as np
import pandas as pd
import pickle

from immunopepper.io_ import decode_utf8
from immunopepper.namedtuples import CountInfo
from immunopepper.namedtuples import GeneInfo
from immunopepper.namedtuples import GeneTable
from immunopepper.namedtuples import ReadingFrameTuple
from immunopepper.utils import encode_chromosome
from immunopepper.utils import find_overlapping_cds_simple
from immunopepper.utils import get_successor_list
from immunopepper.utils import leq_strand
from immunopepper.utils import pool_initializer
from immunopepper.utils import replace_I_with_L


def genes_preprocess_batch(genes, gene_idxs, gene_cds_begin_dict, all_read_frames=False):
    gene_info = []
    for gene in genes:
        gene.from_sparse()
        assert (gene.strand in ["+", "-"])
        assert (len(gene.transcripts) == len(gene.exons))

        # Ignore genes that have no CDS annotated in annotated frame mode
        if (not all_read_frames) and (gene.name not in gene_cds_begin_dict):
            gene_info.append(None)
            continue

        vertex_succ_list = get_successor_list(gene.splicegraph.edges, gene.splicegraph.vertices, gene.strand)
        if gene.strand == "+":
            vertex_order = np.argsort(gene.splicegraph.vertices[0, :])
        else:  # gene.strand=="-"
            vertex_order = np.argsort(gene.splicegraph.vertices[1, :])[::-1]

        # get the reading_frames
        reading_frames = {}
        vertex_len_dict = {}
        if not all_read_frames:
            for idx in vertex_order:
                reading_frames[idx] = list()
                v_start = gene.splicegraph.vertices[0, idx]
                v_stop = gene.splicegraph.vertices[1, idx]
                cds_begins = find_overlapping_cds_simple(v_start, v_stop, gene_cds_begin_dict[gene.name], gene.strand)
                vertex_len_dict[idx] = v_stop - v_start

                # Initialize reading regions from the CDS transcript annotations
                for cds_begin in cds_begins:
                    line_elems = cds_begin[2]
                    cds_strand = line_elems[6]
                    assert (cds_strand == gene.strand)
                    cds_phase = int(line_elems[7])
                    cds_left = int(line_elems[3])-1
                    cds_right = int(line_elems[4])

                    #TODO: need to remove the redundance of (cds_start, cds_stop, item)
                    if gene.strand == "-":
                        cds_right_modi = max(cds_right - cds_phase,v_start)
                        cds_left_modi = v_start
                        n_trailing_bases = cds_right_modi - cds_left_modi
                    else:
                        cds_left_modi = min(cds_left + cds_phase,v_stop)
                        cds_right_modi = v_stop
                        n_trailing_bases = cds_right_modi - cds_left_modi

                    read_phase = n_trailing_bases % 3
                    # add all reading frames from the annotation
                    reading_frame = ReadingFrameTuple(cds_left_modi=cds_left_modi,
                                                              cds_right_modi=cds_right_modi,
                                                              read_phase=read_phase,
                                                              annotated_RF=[True]) # annotated_RF is now mutable
                    if reading_frame not in reading_frames[idx]:
                        reading_frames[idx].append(reading_frame)
        gene.to_sparse()
        gene_info.append(GeneInfo(vertex_succ_list, vertex_order, reading_frames, vertex_len_dict, gene.splicegraph.vertices.shape[1]))

    return gene_info, gene_idxs, genes


def genes_preprocess_all(genes, gene_cds_begin_dict, parallel=1, all_read_frames=False):
    """ Preprocess the gene and generate new attributes under gene object
        Modify the gene object directly

    Parameters
    ----------
    genes: List[Object]. List of gene objects. The object is generated by SplAdder
    gene_cds_begin_dict: Dict. str -> List(int) From gene name to list of cds start positions
    """

    if parallel > 1:
        global genes_info
        global genes_modif
        global cnt
        genes_info = np.zeros((genes.shape[0],), dtype=object)
        genes_modif = np.zeros((genes.shape[0],), dtype=object)
        cnt = 0
        def update_gene_info(result):
            global genes_info
            global cnt
            global genes_modif
            assert(len(result[0]) == len(result[2]))
            for i,tmp in enumerate(result[0]):
                if cnt > 0 and cnt % 1000 == 0:
                    sys.stdout.write('.')
                    if cnt % 10000 == 0:
                        sys.stdout.write('%i/%i\n' % (cnt, genes.shape[0]))
                    sys.stdout.flush()
                cnt += 1
                genes_info[result[1][i]] = tmp
                genes_modif[result[1][i]] = result[2][i]
            del result

        pool = mp.Pool(processes=parallel, initializer=pool_initializer)
        for i in range(0, genes.shape[0], 100):
            gene_idx = np.arange(i, min(i + 100, genes.shape[0]))
            _ = pool.apply_async(genes_preprocess_batch, args=(genes[gene_idx], gene_idx, gene_cds_begin_dict, all_read_frames,), callback=update_gene_info)
        pool.close()
        pool.join()
    else:
        genes_info = genes_preprocess_batch(genes, np.arange(genes.shape[0]), gene_cds_begin_dict, all_read_frames)[0]
        genes_modif = genes
    return genes_info, genes_modif


def preprocess_ann(ann_path):
    """ Extract information from annotation file (.gtf, .gff and .gff3)

    Parameters
    ----------
    ann_path: str. Annotation file path

    Returns
    -------
    gene_table: NamedTuple.store the gene-transcript-cds mapping tables derived
        from .gtf file. has attribute ['gene_to_cds_begin', 'ts_to_cds', 'gene_to_cds']
    chromosome_set: set. Store the chromosome naming.
    """
    transcript_to_gene_dict = {}    # transcript -> gene id
    gene_to_transcript_dict = {}    # gene_id -> list of transcripts
    transcript_to_cds_dict = {}     # transcript -> list of CDS exons
    transcript_cds_begin_dict = {}  # transcript -> first exon of the CDS
    gene_cds_begin_dict = {}        # gene -> list of first CDS exons

    file_type = ann_path.split('.')[-1]
    chromesome_set = set()
    # collect information from annotation file
    for line in open(ann_path, 'r'):
        if line[0] == '#':
            continue
        item = line.strip().split('\t')
        chromesome_set.add(item[0])
        feature_type = item[2]
        attribute_item = item[-1]
        attribute_dict = attribute_item_to_dict(attribute_item, file_type, feature_type)
        # store relationship between gene ID and its transcript IDs
        if feature_type in ['transcript', 'mRNA']:
            gene_id = attribute_dict['gene_id']
            transcript_id = attribute_dict['transcript_id']
            if attribute_dict['gene_type'] != 'protein_coding' or attribute_dict['transcript_type']  != 'protein_coding':
                continue
            assert (transcript_id not in transcript_to_gene_dict)
            transcript_to_gene_dict[transcript_id] = gene_id
            if gene_id in gene_to_transcript_dict and transcript_id not in gene_to_transcript_dict[gene_id]:
                gene_to_transcript_dict[gene_id].append(transcript_id)
            else:
                gene_to_transcript_dict[gene_id] = [transcript_id]

        # Todo python is 0-based while gene annotation file(.gtf, .vcf, .maf) is one based
        elif feature_type == "CDS":
            parent_ts = attribute_dict['transcript_id']
            strand_mode = item[6]
            cds_left = int(item[3])-1
            cds_right = int(item[4])
            frameshift = int(item[7])
            if parent_ts in transcript_to_cds_dict:
                transcript_to_cds_dict[parent_ts].append((cds_left, cds_right, frameshift))
            else:
                transcript_to_cds_dict[parent_ts] = [(cds_left, cds_right, frameshift)]
            if strand_mode == "+" :
                cds_start, cds_stop = cds_left, cds_right
            else:
                cds_start, cds_stop = cds_right, cds_left

            # we only consider the start of the whole CoDing Segment
            if parent_ts not in transcript_cds_begin_dict or \
               leq_strand(cds_start, transcript_cds_begin_dict[parent_ts][0], strand_mode):
                transcript_cds_begin_dict[parent_ts] = (cds_start, cds_stop, item)

    # collect first CDS exons for all transcripts of a gene
    for ts_key in transcript_to_gene_dict:
        target_gene = transcript_to_gene_dict[ts_key]
        if target_gene not in gene_cds_begin_dict:
            gene_cds_begin_dict[target_gene] = []
        if ts_key in transcript_cds_begin_dict:
            gene_cds_begin_dict[target_gene].append(transcript_cds_begin_dict[ts_key])

    # sort list of CDS exons per transcript
    for ts_key in transcript_to_cds_dict:
        transcript_to_cds_dict[ts_key] = sorted(transcript_to_cds_dict[ts_key], key=lambda coordpair: coordpair[0])

    genetable = GeneTable(gene_cds_begin_dict, transcript_to_cds_dict, gene_to_transcript_dict)
    return genetable,chromesome_set


def attribute_item_to_dict(a_item, file_type, feature_type):
    """  From attribute item in annotation file to get corresponding dictionary

    Parameters
    ----------
    a_item: str. attribute item
    file_type: str. Choose from {'gtf', 'gff', 'gff3'}
    feature_type: str. Extract other fields. We only
        consider 'CDS', 'mRNA' and 'transcript'

    Returns
    -------
    gtf_dict: dict. store all the necessary data

    """
    gtf_dict = {}
    if file_type.lower() == 'gtf':
        attribute_list = a_item.split('; ')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split(' ')
            gtf_dict[pair[0]] = pair[1][1:-1]
    elif file_type.lower() == 'gff3':
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gtf_dict[pair[0]] = pair[1]
    elif file_type.lower() == 'gff':
        gff_dict = {}
        attribute_list = a_item.split(';')
        for attribute_pair in attribute_list:
            pair = attribute_pair.split('=')
            gff_dict[pair[0]] = pair[1]  # delete "", currently now work on level 2
        if feature_type == 'CDS':
            gtf_dict['transcript_id'] = gff_dict['Parent']
        elif feature_type in {'mRNA', 'transcript'}:  # mRNA or transcript
            gtf_dict['gene_id'] = gff_dict['geneID']
            gtf_dict['transcript_id'] = gff_dict['ID']
            gtf_dict['gene_type'] = gff_dict['gene_type']
            gtf_dict['transcript_type'] = gff_dict['transcript_type']

    return gtf_dict


def search_edge_metadata_segmentgraph(gene, coord, edge_idxs=None, edge_counts=None, cross_graph_expr=None):
    """Given the ordered edge coordinates of the edge, return expression information of the edge

    Parameters
    ----------
    gene: Object. Generated by SplAdder
    coord: np.array of length 4. Sorted coordinates of 4 positions in ascending order
    countinfo: NamedTuple, contains SplAdder count info
    Idx: Namedtuple, has attribute idx.gene and idx.sample
    edge_idxs: np.array, containing the edge index values for the current gene
    egde_counts: np.array, containing the edge count values for the current gene

    Returns
    -------
    edges_res: tuple of floats. Expression level for the given edges.
    edges_res_metafile: adapted value for the peptide metafile.
    Is nan if matrix mode, because we do not report peptide expressions per sample in this mode
    """
    def get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts=None):
        a = np.searchsorted(segmentgraph.segments[1, :], sorted_pos[1])
        b = np.searchsorted(segmentgraph.segments[0, :], sorted_pos[2])
        if a < b:
            idx = np.ravel_multi_index([a, b], segmentgraph.seg_edges.shape)
        else:
            idx = np.ravel_multi_index([b, a], segmentgraph.seg_edges.shape)
        cidx = np.searchsorted(edge_idxs, idx)
        if len(edge_counts.shape) > 1:
            counts = edge_counts[cidx,:]
        else:
            counts = np.array([edge_counts[cidx]])
        return counts

    edges_res_metafile = np.nan
    segmentgraph = gene.segmentgraph
    sorted_pos = np.sort(np.array([coord.start_v1, coord.stop_v1, coord.start_v2, coord.stop_v2]))

    count = get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts)

    if coord.start_v3 is None:
        edges_res = np.expand_dims(count, axis=0)
    else:
        sorted_pos = np.sort(np.array([coord.start_v2, coord.stop_v2, coord.start_v3, coord.stop_v3]))
        count2 = get_segmentgraph_edge_expr(sorted_pos, edge_idxs, edge_counts)
        edges_res = np.stack([count, count2])

    if not cross_graph_expr:
        edges_res = tuple(i for i in edges_res.flatten()) # TODO Back to tuple for unicity. Needs re-write of "add_peptide_properties"
        edges_res_metafile = edges_res

    return edges_res_metafile, edges_res

def parse_gene_metadata_info(h5fname, sample_list, cross_graph_expr):
    """ Parse the count file

    Parameters
    ----------
    h5fname: str. .count.h5f file
    sample_list: List(str). List of samples.

    Returns
    -------
    countinfo: Namedtuple. Store all the counts information. Has attributes:
        'sample_idx_dict' --> dict from sample name to index
        'gene_idx_dict' --> dict from gene name to index
        'gene_ids_segs' --> array containing segment-geneID relation
        'gene_ids_edges' --> array containing edge-geneID relation
        'h5fname' --> HDF5 file name
    """

    # the SplAdder count hdf5 file has the following structure
    #   h5f["strains"] --> sample names
    #   h5f["segments"] --> segment expression (rows: segments, columns: samples)
    #   h5f["edges"] --> edge expression (rows: edges, columns: samples)
    #   h5f["edge_idx"] --> multi-row index encoding the edge in the splice graph (rows:
    h5f = h5py.File(h5fname, 'r')
    assert (h5f["strains"].shape[0] == h5f["segments"].shape[1])
    assert (h5f["gene_ids_segs"].size ==  h5f["segments"].shape[0])
    assert (h5f["gene_ids_edges"].size == h5f["edges"].shape[0])

    ### create a sample name dictionary mapping sample names to indices
    count_names = h5f['strains'][:] if len(h5f['strains'].shape) == 1 else h5f['strains'][:, 0]
    sample_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(count_names)])

    ### create a gene name dictionary mapping gene names to indices
    gene_names = h5f['gene_names'][:] if len(h5f['gene_names'].shape) == 1 else h5f['gene_names'][:, 0]
    gene_idx_dict = dict([(n.decode('utf8'), i) for i, n in enumerate(gene_names)])

    gene_ids_segs = h5f['gene_ids_segs'][:] if len(h5f['gene_ids_segs'].shape) == 1 else h5f['gene_ids_segs'][:, 0]
    gene_ids_edges = h5f['gene_ids_edges'][:] if len(h5f['gene_ids_edges'].shape) == 1 else h5f['gene_ids_edges'][:, 0]

    ### segs
    gene_ids_segs_u, gene_ids_segs_idx = np.unique(gene_ids_segs, return_index=True)
    gene_ids_segs_idx_last = np.r_[gene_ids_segs_idx[1:], [gene_ids_segs.shape[0]]]
    gene_id_to_segrange = dict()
    for i, g in enumerate(gene_ids_segs_u):
        gene_id_to_segrange[g] = (gene_ids_segs_idx[i], gene_ids_segs_idx_last[i])
    ### edges
    gene_ids_edges_u, gene_ids_edges_idx = np.unique(gene_ids_edges, return_index=True)
    gene_ids_edges_idx_last = np.r_[gene_ids_edges_idx[1:], [gene_ids_edges.shape[0]]]
    gene_id_to_edgerange = dict()
    for i, g in enumerate(gene_ids_edges_u):
        gene_id_to_edgerange[g] = (gene_ids_edges_idx[i], gene_ids_edges_idx_last[i])

    countinfo = CountInfo(sample_idx_dict,
                          gene_idx_dict,
                          gene_id_to_segrange,
                          gene_id_to_edgerange,
                          h5fname)
    h5f.close()
    if cross_graph_expr and sample_list:
        # Retrieve count id matching input samples
        matching_count_ids = np.array([s_idx for input_sample in sample_list for s_idx, graph_sample in enumerate(count_names)
                                  if graph_sample.decode() == input_sample ])
        if countinfo is not None and len(matching_count_ids) == 0:
            logging.error("Output samples do not match count file samples")
            sys.exit(1)
    else:
        matching_count_ids = None

    matching_count_samples = [n.decode('utf8') for n in count_names]
    return countinfo, matching_count_samples, matching_count_ids


# TODO(dd): move mutation parsing methods to mutations.py after review; left here to make code review easier
def parse_mutation_from_vcf(vcf_path: str, mutation_mode: str, mutation_sample: str = None, 
                            graph_to_mutation_samples=dict[str, str], heter_code: int = 0, output_dir: str = None):
    """Extract mutation information from the given vcf or vcf.h5 file
    :param vcf_path: path to the VCF file to be read
    :param mutation_mode: what mutations to apply to the reference. One of 'ref', 'germline', 'somatic',
        'somatic_and_germline'
    :param mutation_sample: TODO(dd) - clarify inconsistency with VCF/HD5 method
    :param graph_to_mutation_samples: dictionary mapping sample id names in the Spladder graph/count files to
        sample ids in the mutation files
    :param heter_code: Can be 0 or 2. specify which number represents the heterozygous allele (HDF5 VCF only)
        0: 0-> homozygous alternative(1|1), 1-> heterozygous(0|1,1|0) 2->homozygous reference(0|0)
        2: 0-> homozygous reference(0|0), 1-> heterozygous(0|1,1|0) 2->homozygous alternative(1|1)
    :param output_dir: location where the pickled result will be cached, if not None
    :return: a dictionary mapping (sample, chromosome) pairs to mutation data, where mutation data is
        a dictionary mapping mutation position, to mutation properties e.g.::
        {('sample1', 'X'): {111: {'ref_base': 'T', 'mut_base': 'C', 'strand': '-',
            'variant_Classification': 'Silent', 'variant_Type': 'SNP'}}}
    :rtype: dict[(str,str):dict[int, dict[str:str]]]
    """

    mutation_to_graph_samples = { file_sample: target_sample for target_sample, file_sample 
                                 in graph_to_mutation_samples.items()}
    if output_dir is not None:
        vcf_pkl_file = os.path.join(output_dir, f'{mutation_mode}_vcf.pickle')
        if os.path.exists(vcf_pkl_file):
            f = open(vcf_pkl_file, 'rb')
            mutation_dict = pickle.load(f)
            pickled_sample_ids = {sample_id for sample_id, chr in mutation_dict}
            _check_mutation_sample_presence(mutation_sample, pickled_sample_ids, graph_to_mutation_samples)
            logging.info(f'Using pickled VCF mutation dict in: {vcf_pkl_file} instead of loading data from: {vcf_path}')
            return mutation_dict

    if not os.path.exists(vcf_path):
        logging.error(f'Could not find mutation file: {vcf_path}')
        sys.exit(1)

    file_type = vcf_path.split('.')[-1]

    if file_type == 'h5':  # hdf5 file
        mutation_dict = parse_mutation_from_vcf_h5(vcf_path, mutation_sample, heter_code,
                                                   graph_to_mutation_samples)
        # TODO(reviewers): why no pickle created?
        logging.info(f'Read germline mutation dict from h5 file in {vcf_path}. No pickle file created')
        return mutation_dict
    
    # vcf text file
    lines = open(vcf_path, 'r').readlines()
    mutation_dict = {}
    for line in lines:
        if line.strip()[:2] == '##':  # annotation line
            continue
        if line.strip()[0] == '#':  # head line
            fields = line.strip().split('\t')
            vcf_sample_set = fields[9:]
            _check_mutation_sample_presence(mutation_sample, vcf_sample_set, graph_to_mutation_samples)
            continue
        items = line.strip().split('\t')
        chromosome = items[0]
        pos = int(items[1]) - 1
        var_dict = {'ref_base': items[3],
                    'mut_base': items[4],
                    'qual': items[5],
                    'filter': items[6]}
        if len(var_dict['ref_base']) == len(var_dict['mut_base']):  # only consider snp for now
            for i, file_sample in enumerate(vcf_sample_set):
                if items[9 + i].split(':')[0] in {'1|1', '1|0', '0|1', '0/1', '1/0', '1/1'}:
                    if file_sample not in mutation_to_graph_samples:
                        mutation_to_graph_samples[file_sample] = file_sample
                    if (mutation_to_graph_samples[file_sample], chromosome) in mutation_dict.keys():
                        mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)][int(pos)] = var_dict
                    else:
                        mutation_dict[(mutation_to_graph_samples[file_sample], chromosome)] = {int(pos): var_dict}
    if output_dir is not None:
        f_pkl = open(vcf_pkl_file, 'wb')
        pickle.dump(mutation_dict, f_pkl)
        logging.info(f'Cached contents of {vcf_path} to {vcf_pkl_file}')
        
    return mutation_dict


def parse_mutation_from_vcf_h5(h5_vcf_path: str, mutation_sample: str, heter_code: int = 0,
                               graph_to_mutation_samples=dict[str, str]):
    """
    Extract germline mutation information from given vcf h5py file.

    Parameters and return value: see :meth:`parse_mutation_from_vcf`
    """
    a = h5py.File(h5_vcf_path, 'r')

    mut_dict = {}

    vcf_sample_set = [decode_utf8(item) for item in a['gtid']]
    _check_mutation_sample_presence(mutation_sample, vcf_sample_set, graph_to_mutation_samples)
    col_id = vcf_sample_set.index(mutation_sample) 
    # the 'gt' column stores the most likely genotype for the sample
    row_id = np.where(np.logical_or(a['gt'][:, col_id] == heter_code, a['gt'][:, col_id] == 1))[0]

    for irow in row_id:
        chromosome = encode_chromosome(a['pos'][irow, 0])
        pos = a['pos'][irow, 1] - 1
        mut_base = decode_utf8(a['allele_alt'][irow])
        ref_base = decode_utf8(a['allele_ref'][irow])
        var_dict = {'mut_base': mut_base, 'ref_base': ref_base}
        if (mutation_sample, chromosome) in mut_dict:
            mut_dict[(mutation_sample, chromosome)][pos] = var_dict
        else:
            mut_dict[(mutation_sample, chromosome)] = {pos: var_dict}
    return mut_dict



def parse_mutation_from_maf(maf_path: str, mutation_mode: str, mutation_sample: str,
                            graph_to_mutation_samples: dict[str, str], output_dir: str = None):
    """
    Extract somatic mutation information from the given MAF file.
    :param mutation_mode: what mutations to apply to the reference.
        One of 'ref', 'germline', 'somatic', 'somatic_and_germline'
    :param mutation_sample:
    :param maf_path: path to the MAF file to be read
    :param output_dir: if not None, directory to cache a pickled version of the mutation dict
    :param graph_to_mutation_samples: dictionary mapping sample id names in the Spladder graph/count files to
        sample ids in the mutation files
    :return: a dictionary mapping (sample, chromosome) pairs to mutation data, where mutation data is
        a dictionary mapping mutation position, to mutation properties e.g.::
        {('sample1', 'X'): {111: {'ref_base': 'T', 'mut_base': 'C', 'strand': '-',
            'variant_Classification': 'Silent', 'variant_Type': 'SNP'}}}
    :rtype: dict[(str,str):dict[int, dict[str:str]]]
    """

    mutation_to_graph_samples = { file_sample: target_sample for target_sample, file_sample
                                  in graph_to_mutation_samples.items()}

    if output_dir:
        maf_pkl_file = os.path.join(output_dir, f'{mutation_mode}_maf.pickle')
        if os.path.exists(maf_pkl_file):  # load pickled mutation dictionary
            f = open(maf_pkl_file, 'rb')
            mutation_dict = pickle.load(f)
            logging.info(f'Using pickled MAF mutation dict in: {maf_pkl_file} instead of loading data from: {maf_path}')
            maf_sample_set = {sample_id for sample_id, chr in mutation_dict}
            _check_mutation_sample_presence(mutation_sample, maf_sample_set, graph_to_mutation_samples)
            return mutation_dict

    if not os.path.exists(maf_path):
        logging.error(f'Could not find mutation file: {maf_path}')
        sys.exit(1)

    lines = open(maf_path).readlines()
    mutation_dict = {}
    maf_sample_set = set()
    for i, line in enumerate(lines[1:]):
        items = line.strip().split('\t')
        if items[9] == 'SNP':  # only consider snp
            file_sample = items[15]
            maf_sample_set.add(file_sample)
            chromosome = items[4]
            pos = int(items[5]) - 1
            var_dict = {
                'ref_base': items[10],
                'mut_base': items[12],
                'strand': items[7],
                'variant_Classification': items[8],
                'variant_Type': items[9]}
            if file_sample not in mutation_to_graph_samples:
                mutation_to_graph_samples[file_sample] = file_sample
            if (mutation_to_graph_samples[file_sample], chromosome) in mutation_dict:
                mutation_dict[((mutation_to_graph_samples[file_sample], chromosome))][int(pos)] = var_dict
            else:
                mutation_dict[((mutation_to_graph_samples[file_sample], chromosome))] = {int(pos): var_dict}

    if output_dir:
        f_pkl = open(maf_pkl_file, 'wb')
        pickle.dump(mutation_dict, f_pkl)
        logging.info(f'Cached contents of {maf_path} to {maf_pkl_file}')

    _check_mutation_sample_presence(mutation_sample, maf_sample_set, graph_to_mutation_samples)
    return mutation_dict


def _check_mutation_sample_presence(mutation_sample, maf_or_vcf_sample_set, graph_to_mutation_samples):
    if mutation_sample and graph_to_mutation_samples[mutation_sample] not in maf_or_vcf_sample_set:
        logging.error(f"Target mutation sample {graph_to_mutation_samples[mutation_sample]} "
                      f"({mutation_sample} in graph/count) is not found in mutation/variant file."
                      f" Please check --mutation-sample or consider using --sample-name-map.")
        logging.error(f"Samples in mutation/variant file are: {maf_or_vcf_sample_set}")
        sys.exit(1)
    if graph_to_mutation_samples:
        for graph_name, mutation_name in graph_to_mutation_samples.items():
            if mutation_name not in maf_or_vcf_sample_set:
                logging.error(f"Sample {mutation_name} ({graph_name} in graph/count) to extract"
                              f" and pickle is not found in mutation/variant file. "
                              f" Please check --pickle-samples or consider using --sample-name-map.")
                logging.error(f"Samples in mutation/variant file are: {maf_or_vcf_sample_set}")
                sys.exit(1)

#todo: support tsv file in the future
def parse_junction_meta_info(h5f_path):
    """ Extract introns of interest from given h5py file

    Parameters
    ----------
    h5f_path: str, h5py file path

    Returns
    -------
    junction_dict: dict, key (chromosome id), value (set of coordinate pairs)

    """
    if h5f_path is None:
        return None
    else:
        h5f = h5py.File(h5f_path,'r')
        chrms = h5f['chrms'][:]
        pos = h5f['pos'][:].astype('str')
        strand = h5f['strand'][:]
        junction_dict = {}

        for i, ichr in enumerate(chrms):
            try:
                junction_dict[decode_utf8(ichr)].add(':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])]))
            except KeyError:
                junction_dict[decode_utf8(ichr)] = set([':'.join([pos[i, 0], pos[i, 1], decode_utf8(strand[i])])])
    return junction_dict


def parse_uniprot(uniprot_path):

    if uniprot_path:
        uniprot = set(pd.read_csv(uniprot_path, header=None)[0])
        return set(map(replace_I_with_L, uniprot))
    else:
        return None

def parse_gene_choices(genes_interest, process_chr, process_num, complexity_cap, disable_process_libsize, graph_data):

    if process_num == 0:  # Default process all genes
        num = len(graph_data)
    else:
        num = process_num
        if num > len(graph_data):
            logging.error(
                "Requested more genes than available in splice graph. Check argument --process_num")
            sys.exit(1)
        graph_data = graph_data[:num]
        disable_process_libsize = True
        logging.warning(
            "Developer mode, processing the first {} genes. Library size will not be outputted.".format(process_num))

    if genes_interest is not None:
        genes_interest = pd.read_csv(genes_interest, header=None)[0].tolist()
        if len(np.array([gene for gene in graph_data if gene.name in genes_interest])) == 0:
            logging.error("Gene of interest not found in splicing graph. Check argument --genes_interest")
            sys.exit(1)
    else:
        genes_interest = [gene.name for gene in graph_data]

    if process_chr is not None:
        gene_with_chr = [gene.name for gene in graph_data if gene.chr in process_chr]
        if len(gene_with_chr) == 0:
            logging.error(
                "Chromosome {} not found in splicing graph. Check argument --process_chr".format(
                    process_chr))
            sys.exit(1)
        genes_interest = [gene for gene in genes_interest if gene in gene_with_chr]
        if len(genes_interest) == 0:
            logging.error(
                "Gene of interest and chromosome of interest do not match. Check argument --genes_interest, --process_chr")
            sys.exit(1)


    if complexity_cap is None or complexity_cap==0: #Default no complexity cap
        complexity_cap = np.inf

    return graph_data, genes_interest, num, complexity_cap, disable_process_libsize


def parse_output_samples_choices(arg, countinfo, matching_count_ids, matching_count_samples):
    '''handle output_sample relatively to output mode '''

    if arg.cross_graph_expr:
        if countinfo:
            process_output_samples = ['cohort']
            # If output samples requested, look for sample ids in count file
            if arg.output_samples:
                arg.output_samples = np.array(arg.output_samples)[np.argsort(matching_count_ids)]
                arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                      for output_sample in  arg.output_samples]
                output_samples_ids = matching_count_ids[np.argsort(matching_count_ids)]
            # If no output samples requested, take all samples in countfile
            else:
                arg.output_samples = [output_sample.replace('-', '').replace('_', '').replace('.', '').replace('/', '')
                                      for output_sample in  matching_count_samples]
                output_samples_ids = None
        else:
            logging.error("Count file must be specified in --cross-graph-exp mode")
            sys.exit(1)
    else:
        if arg.output_samples:
            process_output_samples = arg.output_samples
            output_samples_ids = None
        else:
            logging.error("--arg.output_samples must be specified in single sample mode")
            sys.exit(1)
    return process_output_samples, output_samples_ids
